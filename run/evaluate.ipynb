{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import anndata as ad\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "USE_ADT2GEX = False\n",
    "if USE_ADT2GEX:\n",
    "    #adt2gex\n",
    "    test_path = \"../datasets/openproblems_bmmc_cite_phase2_rna/openproblems_bmmc_cite_phase2_rna.censor_dataset.output_\"\n",
    "else:\n",
    "    #atac2gex\n",
    "    test_path = \"../datasets/openproblems_bmmc_multiome_phase2_rna/openproblems_bmmc_multiome_phase2_rna.censor_dataset.output_\"\n",
    "\n",
    "# change this to the matching prediction file path\n",
    "prediction_path = \"../pretrain/pbmc1NoEGEX2ATAC.h5ad\"\n",
    "\n",
    "par = {\n",
    "    \"input_test_prediction\": prediction_path,\n",
    "    \"input_test_sol\": f\"{test_path}test_sol.h5ad\",\n",
    "}\n",
    "\n",
    "prediction_test = ad.read_h5ad(par[\"input_test_prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if type(prediction_test.X) != np.ndarray:\n",
    "    X = prediction_test.X.toarray()\n",
    "else:\n",
    "    X = prediction_test.X\n",
    "X = torch.tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# run this if the two datasets are not aligned and there is a ground truth matching matrix\n",
    "sol_test = ad.read_h5ad(par[\"input_test_sol\"])\n",
    "Xsol = torch.tensor(sol_test.X.toarray())\n",
    "Xsol.argmax(1)\n",
    "# Order the columns of the prediction matrix so that the perfect prediction is the identity matrix\n",
    "X = X[:, Xsol.argmax(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = torch.arange(X.shape[0])\n",
    "forward_accuracy = (torch.argmax(X, dim=1) == labels).float().mean().item()\n",
    "backward_accuracy = (\n",
    "    (torch.argmax(X, dim=0) == labels).float().mean().item()\n",
    ")\n",
    "avg_accuracy = 0.5 * (forward_accuracy + backward_accuracy)\n",
    "print(forward_accuracy, backward_accuracy, \"top1-acc:\", avg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, top_indexes_forward = X.topk(5, dim=1)\n",
    "_, top_indexes_backward = X.topk(5, dim=0)\n",
    "l_forward = labels.expand(5, X.shape[0]).T\n",
    "l_backward = l_forward.T\n",
    "top5_forward_accuracy = (\n",
    "    torch.any(top_indexes_forward == l_forward, 1).float().mean().item()\n",
    ")\n",
    "top5_backward_accuracy = (\n",
    "    torch.any(top_indexes_backward == l_backward, 0).float().mean().item()\n",
    ")\n",
    "top5_avg_accuracy = 0.5 * (top5_forward_accuracy + top5_backward_accuracy)\n",
    "\n",
    "print(top5_forward_accuracy, top5_backward_accuracy, \"top5-acc:\", top5_avg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top_indexes_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(top_indexes_forward[:, 1:] != torch.Tensor([1,3,4,0])).float().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.any(top_indexes_forward == l_forward, 1).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### FOSCTTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"FOSCTTM:\", (X > torch.diag(X)).float().mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "foscttm_x = (X >= torch.diag(X)).float().mean(axis=1)\n",
    "foscttm_y = (X >= torch.diag(X)).float().mean(axis=0)\n",
    "# foscttm_y = (d < np.expand_dims(np.diag(d), axis=0)).mean(axis=0)\n",
    "print(\"foscttm_x\", foscttm_x, \"foscttm_y\", foscttm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "foscttm_y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### For soft predictions, the competition score can be made equal to the forward accuracy (or backward accuracy) by putting 1 at the max of each row (or each column) and 0 elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logits_row_sums = X.clip(min=0).sum(dim=1)\n",
    "top1_competition_metric = X.clip(min=0).diag().div(logits_row_sums).mean().item()\n",
    "print(\"Top-1 competition metric for hard matching predictions:\", top1_competition_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mx = torch.max(X, dim=1, keepdim=True).values\n",
    "hard_X = (mx == X).float()\n",
    "logits_row_sums = hard_X.clip(min=0).sum(dim=1)\n",
    "top1_competition_metric = hard_X.clip(min=0).diagonal().div(logits_row_sums).mean().item()\n",
    "print(\"Top-1 competition metric for soft matching predictions: \", top1_competition_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save hard_X\n",
    "with open(\"hard_X.npy\", \"wb\") as f:\n",
    "    np.save(f, hard_X.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "28a8c7923e5120ca4bc6ada9b441fd16ffdb1553ab080deef118f19b45e91ea2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}